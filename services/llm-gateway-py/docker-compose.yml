version: '3.8'

services:
  llm-gateway-py:
    build:
      context: ../..
      dockerfile: services/llm-gateway-py/Dockerfile
    ports:
      - "50054:50054"  # gRPC port
      - "8091:8091"    # HTTP admin port
    environment:
      # Service configuration
      - SERVICE_NAME=llm-gateway-py
      - ENVIRONMENT=development
      - DEBUG=true
      - LOG_LEVEL=DEBUG
      
      # Server configuration
      - GRPC_PORT=50054
      - HTTP_PORT=8091
      - MAX_WORKERS=10
      
      # Admin API
      - ENABLE_ADMIN_API=true
      - ADMIN_API_KEY=admin-secret-key
      
      # External APIs (set these in .env or as environment variables)
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - PINECONE_API_KEY=${PINECONE_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      
      # Pinecone configuration
      - PINECONE_ENVIRONMENT=${PINECONE_ENVIRONMENT:-us-east-1-aws}
      - PINECONE_INDEX=${PINECONE_INDEX:-university-scores}
      
      # Feature flags
      - WEB_SEARCH_ENABLED=true
    volumes:
      - llm_logs:/app/logs
    networks:
      - llm-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; grpc.channel_ready_future(grpc.insecure_channel('localhost:50054')).result(timeout=10)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

volumes:
  llm_logs:

networks:
  llm-network:
    driver: bridge
