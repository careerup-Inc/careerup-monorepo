# Environment configuration for LLM Gateway Python service
# Copy this file to .env and update the values

# Service Configuration
SERVICE_NAME=llm-gateway-py
ENVIRONMENT=development
DEBUG=true
LOG_LEVEL=INFO

# Server Configuration
GRPC_PORT=50054
HTTP_PORT=8091
MAX_WORKERS=10

# Admin API Configuration
ENABLE_ADMIN_API=true
ADMIN_API_KEY=admin-secret-key-change-me

# External API Keys (Required)
OPENAI_API_KEY=your-openai-api-key-here
PINECONE_API_KEY=your-pinecone-api-key-here
TAVILY_API_KEY=your-tavily-api-key-here

# Pinecone Configuration
PINECONE_ENVIRONMENT=us-east-1-aws
PINECONE_INDEX=university-scores

# Feature Flags
WEB_SEARCH_ENABLED=true

# Logging Configuration
LOG_FILE=/app/logs/llm-gateway.log

# Performance Tuning
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200
RAG_RETRIEVAL_TOP_K=5
RAG_TEMPERATURE=0.7
RAG_MAX_TOKENS=1000
RAG_MAX_RETRIES=3

# Vector Store Configuration
EMBEDDING_MODEL=text-embedding-ada-002
EMBEDDING_DIMENSIONS=1536
